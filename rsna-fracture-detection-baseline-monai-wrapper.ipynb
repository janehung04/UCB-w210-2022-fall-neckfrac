{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d39746-a43e-42ee-883d-6401f7445281",
   "metadata": {},
   "source": [
    "# Wrapper for Baseline MONAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f15e79e-9eee-4610-b08c-f06dba108063",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/usr/bin/env python\u001b[39;49;00m\n",
      "\u001b[37m# coding: utf-8\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m## Load Packages\u001b[39;49;00m\n",
      "\n",
      "\u001b[04m\u001b[91m!\u001b[39;49;00mpip install warmup_scheduler\n",
      "\u001b[04m\u001b[91m!\u001b[39;49;00mpip install -U pydicom\n",
      "\u001b[04m\u001b[91m!\u001b[39;49;00mpip install albumentations\n",
      "\u001b[04m\u001b[91m!\u001b[39;49;00mpip install monai\n",
      "\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgc\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtqdm\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m tqdm\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pprint\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m time\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwarnings\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcv2\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Image\n",
      "\n",
      "\u001b[37m# .dcm handling\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpydicom\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# PyTorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TensorDataset, DataLoader, Dataset\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m lr_scheduler\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msampler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SubsetRandomSampler, RandomSampler, SequentialSampler\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlr_scheduler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtransforms\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtransforms\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mwarmup_scheduler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GradualWarmupScheduler\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36malbumentations\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# MONAI 3D\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmonai\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtransforms\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Randomizable, apply_transform\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmonai\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtransforms\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Compose, Resize, ScaleIntensity, ToTensor, RandAffine\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmonai\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnetworks\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m densenet\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GroupKFold\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m roc_auc_score\n",
      "\n",
      "\u001b[37m# Environment check\u001b[39;49;00m\n",
      "warnings.filterwarnings(\u001b[33m\"\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m### Helper Function\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mset_seed\u001b[39;49;00m(seed=\u001b[34m0\u001b[39;49;00m):\n",
      "    random.seed(seed)\n",
      "    np.random.seed(seed)\n",
      "    torch.manual_seed(seed)  \n",
      "    torch.cuda.manual_seed(seed)  \n",
      "    torch.cuda.manual_seed_all(seed)  \n",
      "    torch.backends.cudnn.deterministic = \u001b[34mTrue\u001b[39;49;00m\n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32matoi\u001b[39;49;00m(text):\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mint\u001b[39;49;00m(text) \u001b[34mif\u001b[39;49;00m text.isdigit() \u001b[34melse\u001b[39;49;00m text\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mnatural_keys\u001b[39;49;00m(text):\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    alist.sort(key=natural_keys) sorts in human order\u001b[39;49;00m\n",
      "\u001b[33m    http://nedbatchelder.com/blog/200712/human_sorting.html\u001b[39;49;00m\n",
      "\u001b[33m    (See Toothy's implementation in the comments)\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m [ atoi(c) \u001b[34mfor\u001b[39;49;00m c \u001b[35min\u001b[39;49;00m re.split(\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m(\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\u001b[33md+)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, text) ]\n",
      "\n",
      "\u001b[37m# # some patients have reverse order for the CT scan, so have a function to check\u001b[39;49;00m\n",
      "\u001b[37m# def check_reverse_required(path):\u001b[39;49;00m\n",
      "\u001b[37m#     paths = list(path.glob('*'))\u001b[39;49;00m\n",
      "\u001b[37m#     paths.sort(key=lambda x:int(x.stem))\u001b[39;49;00m\n",
      "\u001b[37m#     z_first = pydicom.dcmread(paths[0]).get(\"ImagePositionPatient\")[-1]\u001b[39;49;00m\n",
      "\u001b[37m#     z_last = pydicom.dcmread(paths[-1]).get(\"ImagePositionPatient\")[-1]\u001b[39;49;00m\n",
      "\u001b[37m#     if z_last < z_first:\u001b[39;49;00m\n",
      "\u001b[37m#         return False\u001b[39;49;00m\n",
      "\u001b[37m#     return True\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# paths = {\u001b[39;49;00m\n",
      "\u001b[37m#     'train': Path('../input/rsna-2022-cervical-spine-fracture-detection/train.csv'),\u001b[39;49;00m\n",
      "\u001b[37m#     'train_bbox': Path('../input/rsna-2022-cervical-spine-fracture-detection/train_bounding_boxes.csv'),\u001b[39;49;00m\n",
      "\u001b[37m#     'train_images': Path('../input/rsna-2022-cervical-spine-fracture-detection/train_images'),\u001b[39;49;00m\n",
      "\u001b[37m#     'train_nifti_segments': Path('../input/rsna-2022-cervical-spine-fracture-detection/segmentations'),\u001b[39;49;00m\n",
      "\u001b[37m#     'test_df': Path('../input/rsna-2022-cervical-spine-fracture-detection/test.csv'),\u001b[39;49;00m\n",
      "\u001b[37m#     'test_images': Path('../input/rsna-2022-cervical-spine-fracture-detection/test_images')\u001b[39;49;00m\n",
      "\u001b[37m# }\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[37m# custom weighted loss function\u001b[39;49;00m\n",
      "\u001b[37m# From: https://www.kaggle.com/code/andradaolteanu/rsna-fracture-detect-pytorch-densenet-train#2.-Data-Split\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_custom_loss\u001b[39;49;00m(logits, targets):\n",
      "    \n",
      "    \u001b[37m# Compute the weights\u001b[39;49;00m\n",
      "    weights = targets * competition_weights[\u001b[33m'\u001b[39;49;00m\u001b[33m+\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] + (\u001b[34m1\u001b[39;49;00m - targets) * competition_weights[\u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    \u001b[37m# Losses on label and exam level\u001b[39;49;00m\n",
      "    L = torch.zeros(targets.shape, device=DEVICE)\n",
      "\n",
      "    w = weights\n",
      "    y = targets\n",
      "    p = logits\n",
      "    eps=\u001b[34m1e-8\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(L.shape[\u001b[34m0\u001b[39;49;00m]):\n",
      "        \u001b[34mfor\u001b[39;49;00m j \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(L.shape[\u001b[34m1\u001b[39;49;00m]):\n",
      "            L[i, j] = -w[i, j] * (\n",
      "                y[i, j] * math.log(p[i, j] + eps) +\n",
      "                (\u001b[34m1\u001b[39;49;00m - y[i, j]) * math.log(\u001b[34m1\u001b[39;49;00m - p[i, j] + eps))\n",
      "            \n",
      "    \u001b[37m# Average Loss on Exam (or patient)\u001b[39;49;00m\n",
      "    Exams_Loss = torch.div(torch.sum(L, dim=\u001b[34m1\u001b[39;49;00m), torch.sum(w, dim=\u001b[34m1\u001b[39;49;00m))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m Exams_Loss\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mRSNADataset\u001b[39;49;00m(Dataset, Randomizable):\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, csv, mode, transform=\u001b[34mNone\u001b[39;49;00m):\n",
      "        \u001b[36mself\u001b[39;49;00m.csv = csv\n",
      "        \u001b[36mself\u001b[39;49;00m.mode = mode\n",
      "        \u001b[36mself\u001b[39;49;00m.transform = transform\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.csv.shape[\u001b[34m0\u001b[39;49;00m]\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mrandomize\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[33m'''-> None is a type annotation for the function that states \u001b[39;49;00m\n",
      "\u001b[33m        that this function returns None.'''\u001b[39;49;00m\n",
      "        \n",
      "        MAX_SEED = np.iinfo(np.uint32).max + \u001b[34m1\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.seed = \u001b[36mself\u001b[39;49;00m.R.randint(MAX_SEED, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33muint32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, index):\n",
      "        \u001b[37m# Set Random Seed\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.randomize()\n",
      "        \n",
      "        dt = \u001b[36mself\u001b[39;49;00m.csv.iloc[index, :]\n",
      "        study_paths = glob(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mDATA_PATH\u001b[33m}\u001b[39;49;00m\u001b[33m/train_images/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdt.StudyInstanceUID\u001b[33m}\u001b[39;49;00m\u001b[33m/*\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        study_paths.sort(key=natural_keys)\n",
      "        \n",
      "        \u001b[37m# Load images\u001b[39;49;00m\n",
      "        study_images = [cv2.imread(path)[:,:,::-\u001b[34m1\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m path \u001b[35min\u001b[39;49;00m study_paths]\n",
      "        \n",
      "        \u001b[37m# Stack all scans into 1\u001b[39;49;00m\n",
      "        stacked_image = np.stack([img.astype(np.float32) \u001b[34mfor\u001b[39;49;00m img \u001b[35min\u001b[39;49;00m study_images], \n",
      "                                 axis=\u001b[34m2\u001b[39;49;00m).transpose(\u001b[34m3\u001b[39;49;00m,\u001b[34m0\u001b[39;49;00m,\u001b[34m1\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.transform:\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.transform, Randomizable):\n",
      "                \u001b[36mself\u001b[39;49;00m.transform.set_random_state(seed=\u001b[36mself\u001b[39;49;00m.seed)\n",
      "                \n",
      "            stacked_image = apply_transform(\u001b[36mself\u001b[39;49;00m.transform, stacked_image)\n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.mode==\u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "            \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: stacked_image}\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            targets = torch.tensor(dt[target_cols]).float()\n",
      "            \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: stacked_image,\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mtargets\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: targets}\n",
      "\n",
      "\u001b[37m# send the data to GPU\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdata_to_device\u001b[39;49;00m(data):\n",
      "    image, targets = data.values()\n",
      "    \u001b[34mreturn\u001b[39;49;00m image.to(DEVICE), targets.to(DEVICE)\n",
      "\n",
      "\u001b[37m## Loss & Gradual warmup\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Reference link: [HERE](https://stackoverflow.com/questions/42479902/what-does-view-do-in-pytorch)\u001b[39;49;00m\n",
      "\u001b[37m# \u001b[39;49;00m\n",
      "\u001b[37m# torch.view(-1):\u001b[39;49;00m\n",
      "\u001b[37m# * view() reshapes the tensor without copying memory, similar to numpy's reshape().\u001b[39;49;00m\n",
      "\u001b[37m# * -1 flatten the tensor\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_criterion\u001b[39;49;00m(logits, target): \n",
      "    loss = CRITERION(logits.view(-\u001b[34m1\u001b[39;49;00m), target.view(-\u001b[34m1\u001b[39;49;00m))\n",
      "    \u001b[34mreturn\u001b[39;49;00m loss\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mGradualWarmupSchedulerV2\u001b[39;49;00m(GradualWarmupScheduler):\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    src: https://www.kaggle.com/code/boliu0/monai-3d-cnn-training/notebook\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, optimizer, multiplier, total_epoch, after_scheduler=\u001b[34mNone\u001b[39;49;00m):\n",
      "        \u001b[36msuper\u001b[39;49;00m(GradualWarmupSchedulerV2, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m(optimizer, multiplier, \n",
      "                                                       total_epoch, after_scheduler)\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_lr\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.last_epoch > \u001b[36mself\u001b[39;49;00m.total_epoch:\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.after_scheduler:\n",
      "                \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.finished:\n",
      "                    \u001b[36mself\u001b[39;49;00m.after_scheduler.base_lrs = [base_lr * \u001b[36mself\u001b[39;49;00m.multiplier \n",
      "                                                     \u001b[34mfor\u001b[39;49;00m base_lr \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.base_lrs]\n",
      "                    \u001b[36mself\u001b[39;49;00m.finished = \u001b[34mTrue\u001b[39;49;00m\n",
      "                \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.after_scheduler.get_lr()\n",
      "            \u001b[34mreturn\u001b[39;49;00m [base_lr * \u001b[36mself\u001b[39;49;00m.multiplier \u001b[34mfor\u001b[39;49;00m base_lr \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.base_lrs]\n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.multiplier == \u001b[34m1.0\u001b[39;49;00m:\n",
      "            \u001b[34mreturn\u001b[39;49;00m [base_lr * (\u001b[36mfloat\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.last_epoch) / \u001b[36mself\u001b[39;49;00m.total_epoch) \n",
      "                    \u001b[34mfor\u001b[39;49;00m base_lr \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.base_lrs]\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[34mreturn\u001b[39;49;00m [base_lr * ((\u001b[36mself\u001b[39;49;00m.multiplier - \u001b[34m1.\u001b[39;49;00m) * \u001b[36mself\u001b[39;49;00m.last_epoch / \u001b[36mself\u001b[39;49;00m.total_epoch + \u001b[34m1.\u001b[39;49;00m) \n",
      "                    \u001b[34mfor\u001b[39;49;00m base_lr \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.base_lrs]\n",
      "\n",
      "\u001b[37m## Log the info\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32madd_in_file\u001b[39;49;00m(text, f):\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mlog_\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mKERNEL_TYPE\u001b[33m}\u001b[39;49;00m\u001b[33m.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ma+\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        \u001b[36mprint\u001b[39;49;00m(text, file=f)\n",
      "\n",
      "\u001b[37m## Train Epoch\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_epoch\u001b[39;49;00m(model, dataloader, optimizer, epoch, f):\n",
      "    \n",
      "    \u001b[37m# Add info to file\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    add_in_file(\u001b[33m'\u001b[39;49;00m\u001b[33mTraining...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, f)\n",
      "    \n",
      "    \u001b[37m# Track training time for 1 epoch\u001b[39;49;00m\n",
      "    start_time = time()\n",
      "    \n",
      "    \u001b[37m# === TRAIN ===\u001b[39;49;00m\n",
      "    model.train()\n",
      "    train_losses, train_comp_losses = [], []\n",
      "    \n",
      "    \u001b[37m# Loop through the data\u001b[39;49;00m\n",
      "    bar = tqdm(dataloader)\n",
      "    \u001b[34mfor\u001b[39;49;00m data \u001b[35min\u001b[39;49;00m bar:\n",
      "        image, targets = data_to_device(data)\n",
      "        \n",
      "        \u001b[37m# Train & Optimize\u001b[39;49;00m\n",
      "        optimizer.zero_grad()\n",
      "        logits = model(image)\n",
      "        loss = get_criterion(logits, targets)\n",
      "        loss.sum().backward()\n",
      "        optimizer.step()\n",
      "        \n",
      "        \u001b[37m# === COMP LOSS ===\u001b[39;49;00m\n",
      "        comp_loss = get_custom_loss(logits, targets)\n",
      "\n",
      "        \u001b[37m# Save losses\u001b[39;49;00m\n",
      "        train_losses.append(loss.detach().cpu().numpy())\n",
      "        train_comp_losses.append(comp_loss.detach().cpu().numpy().mean())\n",
      "        \n",
      "        gc.collect()\n",
      "\n",
      "    \u001b[37m# Compute Overall Loss\u001b[39;49;00m\n",
      "    mean_train_loss = np.mean(train_losses)\n",
      "    mean_comp_loss = np.mean(train_comp_losses)\n",
      "    \n",
      "    \u001b[37m# Save info\u001b[39;49;00m\n",
      "    total_time = \u001b[36mround\u001b[39;49;00m((time() - start_time)/\u001b[34m60\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m)\n",
      "    add_in_file(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain Mean Loss: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(mean_train_loss), f)\n",
      "    add_in_file(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain Mean Comp Loss: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(mean_comp_loss), f)\n",
      "    add_in_file(\u001b[33m'\u001b[39;49;00m\u001b[33m~~~ Train Time: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m mins ~~~\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(total_time), f)\n",
      "                \n",
      "    \u001b[37m# Print info\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Mean Loss:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mean_train_loss)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Mean Comp Loss:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mean_comp_loss)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m~~~ Train Time: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtotal_time\u001b[33m}\u001b[39;49;00m\u001b[33m mins ~~~\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m mean_train_loss\n",
      "\n",
      "\u001b[37m## Validation Epoch\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mvalid_epoch\u001b[39;49;00m(model, dataloader, epoch, f):\n",
      "    \n",
      "    \u001b[37m# Add info to file\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mValidation...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    add_in_file(\u001b[33m'\u001b[39;49;00m\u001b[33mValidation...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, f)\n",
      "    \n",
      "    \u001b[37m# Track validation time for 1 epoch\u001b[39;49;00m\n",
      "    start_time = time()\n",
      "    \n",
      "    \u001b[37m# === EVAL ===\u001b[39;49;00m\n",
      "    model.eval()\n",
      "    valid_preds, valid_targets, valid_comp_loss = [], [], []\n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m data \u001b[35min\u001b[39;49;00m dataloader:\n",
      "            \n",
      "            image, targets = data_to_device(data)\n",
      "            logits = model(image)\n",
      "            \n",
      "            \u001b[37m# === COMP LOSS ===\u001b[39;49;00m\n",
      "            comp_loss = get_custom_loss(logits, targets)\n",
      "            \u001b[37m# Save actuals, preds and losses\u001b[39;49;00m\n",
      "            valid_targets.append(targets.detach().cpu())\n",
      "            valid_preds.append(logits.detach().cpu())\n",
      "            valid_comp_loss.append(comp_loss.detach().cpu().numpy().mean())\n",
      "            \n",
      "            gc.collect()\n",
      "            \n",
      "    \u001b[37m# Overall Valid Loss\u001b[39;49;00m\n",
      "    valid_losses = get_criterion(torch.cat(valid_preds), torch.cat(valid_targets)).numpy()\n",
      "    mean_valid_loss = np.mean(valid_losses)\n",
      "    \n",
      "    \u001b[37m# Overall Competition Loss\u001b[39;49;00m\n",
      "    mean_comp_valid_loss = np.mean(valid_comp_loss)\n",
      "    \n",
      "    \u001b[37m# Compute Area Under Curve\u001b[39;49;00m\n",
      "    PREDS = np.concatenate(torch.cat(valid_preds).numpy())\n",
      "    TARGETS = np.concatenate(torch.cat(valid_targets).numpy())\n",
      "    auc = roc_auc_score(TARGETS, PREDS)\n",
      "    \n",
      "    \u001b[37m# Save info\u001b[39;49;00m\n",
      "    total_time = \u001b[36mround\u001b[39;49;00m((time() - start_time)/\u001b[34m60\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m)\n",
      "    add_in_file(\u001b[33m'\u001b[39;49;00m\u001b[33mValid Mean Loss: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(mean_valid_loss), f)\n",
      "    add_in_file(\u001b[33m'\u001b[39;49;00m\u001b[33mValid Mean Comp Loss: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(mean_comp_valid_loss), f)\n",
      "    add_in_file(\u001b[33m'\u001b[39;49;00m\u001b[33mValid AUC: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(auc), f)\n",
      "    add_in_file(\u001b[33m'\u001b[39;49;00m\u001b[33m~~~ Valid Time: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m mins ~~~\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(total_time), f)\n",
      "        \n",
      "    \u001b[37m# Print info\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mValid Mean Loss:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mean_valid_loss)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mValid Mean Comp Loss:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mean_comp_valid_loss)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mValid AUC:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, auc)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m~~~ Validation Time: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtotal_time\u001b[33m}\u001b[39;49;00m\u001b[33m mins ~~~\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m mean_valid_loss\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mrun_train\u001b[39;49;00m(fold):\n",
      "    \n",
      "    \u001b[37m# Get the train and valid data\u001b[39;49;00m\n",
      "    train = df[df[\u001b[33m\"\u001b[39;49;00m\u001b[33mfold\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] != fold].reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    valid = df[df[\u001b[33m\"\u001b[39;49;00m\u001b[33mfold\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] == fold].reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Create the Dataset & Dataloader\u001b[39;49;00m\n",
      "    train_dataset = RSNADataset(csv=train, mode=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "                                transform=train_transforms)\n",
      "    valid_dataset = RSNADataset(csv=valid, mode=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "                                transform=valid_transforms)\n",
      "    trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
      "                             sampler=RandomSampler(train_dataset), num_workers=NUM_WORKERS)\n",
      "    validloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
      "    \n",
      "    \u001b[37m# Model\u001b[39;49;00m\n",
      "    model = densenet.densenet121(spatial_dims=\u001b[34m3\u001b[39;49;00m, in_channels=\u001b[34m3\u001b[39;49;00m,\n",
      "                                 out_channels=OUT_DIM)\n",
      "    model.class_layers.out = nn.Sequential(nn.Linear(in_features=\u001b[34m1024\u001b[39;49;00m, out_features=OUT_DIM), \n",
      "                                           nn.Softmax(dim=\u001b[34m1\u001b[39;49;00m))\n",
      "    model.to(DEVICE)\n",
      "    \n",
      "    \u001b[37m# Optimizer & Scheduler\u001b[39;49;00m\n",
      "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
      "    scheduler_cosine = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \u001b[34m2\u001b[39;49;00m)\n",
      "    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=\u001b[34m10\u001b[39;49;00m, \n",
      "                                                total_epoch=\u001b[34m1\u001b[39;49;00m, \n",
      "                                                after_scheduler=scheduler_cosine)\n",
      "    \n",
      "    \u001b[37m# Initiate initial loss\u001b[39;49;00m\n",
      "    valid_loss_BEST = \u001b[34m1000\u001b[39;49;00m\n",
      "    \u001b[37m# Create model name\u001b[39;49;00m\n",
      "    model_file = \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mKERNEL_TYPE\u001b[33m}\u001b[39;49;00m\u001b[33m_best_fold\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mfold\u001b[33m}\u001b[39;49;00m\u001b[33m.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    \u001b[37m# Create file to save outputs\u001b[39;49;00m\n",
      "    f = \u001b[36mopen\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mlog_\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mKERNEL_TYPE\u001b[33m}\u001b[39;49;00m\u001b[33m.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ma\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(EPOCHS):\n",
      "        \n",
      "        add_in_file(\u001b[33m'\u001b[39;49;00m\u001b[33m======== Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ========\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epoch+\u001b[34m1\u001b[39;49;00m, EPOCHS), f)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m*\u001b[34m8\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m*\u001b[34m8\u001b[39;49;00m)\n",
      "        \n",
      "        scheduler_warmup.step(epoch-\u001b[34m1\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[37m# Train & Validate\u001b[39;49;00m\n",
      "        mean_train_loss = train_epoch(model, trainloader, optimizer, epoch, f)\n",
      "        mean_valid_loss = valid_epoch(model, validloader, epoch, f)\n",
      "        \n",
      "        \u001b[37m# Save model\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m mean_valid_loss < valid_loss_BEST:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving model ...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            add_in_file(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving model => \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model_file), f)\n",
      "            torch.save(model.state_dict(), model_file)\n",
      "            valid_loss_BEST = mean_valid_loss\n",
      "            \n",
      "    torch.cuda.empty_cache()\n",
      "    gc.collect()\n",
      "\n",
      "\n",
      "\n",
      "\u001b[37m## Reference\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# * [RSNA Fracture Detect: PyTorch DenseNet train](https://www.kaggle.com/code/andradaolteanu/rsna-fracture-detect-pytorch-densenet-train)\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# set seed\u001b[39;49;00m\n",
      "    set_seed(\u001b[34m0\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# set GPU\u001b[39;49;00m\n",
      "    DEVICE = torch.device(\u001b[33m'\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDevice:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, DEVICE)\n",
      "\n",
      "    \u001b[37m## Set up parameters\u001b[39;49;00m\n",
      "    DF_SIZE = \u001b[34m.20\u001b[39;49;00m\n",
      "    N_SPLITS = \u001b[34m5\u001b[39;49;00m\n",
      "    KERNEL_TYPE = \u001b[33m'\u001b[39;49;00m\u001b[33mdensenet121_baseline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    IMG_RESIZE = \u001b[34m150\u001b[39;49;00m\n",
      "    STACK_RESIZE = \u001b[34m50\u001b[39;49;00m\n",
      "    use_amp = \u001b[34mFalse\u001b[39;49;00m\n",
      "    NUM_WORKERS = \u001b[34m0\u001b[39;49;00m \u001b[37m#mp.cpu_count()\u001b[39;49;00m\n",
      "    BATCH_SIZE = \u001b[34m8\u001b[39;49;00m \u001b[37m#16\u001b[39;49;00m\n",
      "    LR = \u001b[34m0.0005\u001b[39;49;00m\n",
      "    OUT_DIM = \u001b[34m8\u001b[39;49;00m\n",
      "    EPOCHS = \u001b[34m5\u001b[39;49;00m\n",
      "    DATA_PATH = \u001b[33m\"\u001b[39;49;00m\u001b[33m/root/input/rsna-2022-cervical-spine-fracture-detection\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "    target_cols = [\u001b[33m'\u001b[39;49;00m\u001b[33mC1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mC2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mC3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mC4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mC5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mC6\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mC7\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mpatient_overall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    competition_weights = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m : torch.tensor([\u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, \u001b[34m7\u001b[39;49;00m], dtype=torch.float, device=DEVICE),\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33m+\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m : torch.tensor([\u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m14\u001b[39;49;00m], dtype=torch.float, device=DEVICE),\n",
      "    }\n",
      "\n",
      "    \u001b[37m# Load Data\u001b[39;49;00m\n",
      "    df = pd.read_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mDATA_PATH\u001b[33m}\u001b[39;49;00m\u001b[33m/train.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Sample down df\u001b[39;49;00m\n",
      "    instances = df.StudyInstanceUID.unique().tolist()\n",
      "    instances = random.sample(instances, k=\u001b[36mint\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(instances)*DF_SIZE))\n",
      "    df = df[df[\u001b[33m\"\u001b[39;49;00m\u001b[33mStudyInstanceUID\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].isin(instances)].reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDataframe size:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, df.shape)\n",
      "\n",
      "    \u001b[37m# Create folds\u001b[39;49;00m\n",
      "    kfold = GroupKFold(n_splits=N_SPLITS)\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mfold\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = -\u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Append fold\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m k, (_, valid_i) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(kfold.split(df,\n",
      "                                                 groups=df.StudyInstanceUID)):\n",
      "        df.loc[valid_i, \u001b[33m'\u001b[39;49;00m\u001b[33mfold\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = k\n",
      "    \n",
      "    CRITERION = nn.BCEWithLogitsLoss(reduction=\u001b[33m'\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# transform\u001b[39;49;00m\n",
      "    train_transforms = Compose([ScaleIntensity(), \n",
      "                                Resize((IMG_RESIZE, IMG_RESIZE, STACK_RESIZE)), \n",
      "                                ToTensor()])\n",
      "    valid_transforms = Compose([ScaleIntensity(), \n",
      "                              Resize((IMG_RESIZE, IMG_RESIZE, STACK_RESIZE)), \n",
      "                              ToTensor()])\n",
      "    \n",
      "    \n",
      "    \u001b[37m# Train\u001b[39;49;00m\n",
      "    run_train(fold=\u001b[34m0\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize rsna-fracture-detection-baseline-monai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441324a2-193a-4c68-9c1a-9df9039692cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4953a023-aff7-49da-9f6e-7fd3a7781ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf7b9f1-5c98-447e-bf89-3b4d82af6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"rsna-fracture-detection-baseline-monai.py\",\n",
    "    role=role,\n",
    "    py_version=\"py38\",\n",
    "    framework_version=\"1.11.0\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    # hyperparameters={\"epochs\": 1, \"backend\": \"gloo\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85682f16-2a3e-44c5-b992-5eb590e42050",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g4dn.xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-28be9b2c12b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_default_rule_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \"\"\"Set default rule configurations.\n\u001b[0;32m--> 950\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mrule\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRuleBase\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0mrule\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mderives\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mRuleBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1758\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0menable_network_isolation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m                 \u001b[0menable_network_isolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_network_isolation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m             model = self.create_model(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0mmetric_definitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_definitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m             \u001b[0menable_network_isolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_network_isolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0mimage_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0malgorithm_arn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgorithm_arn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g4dn.xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e4a333-9f09-48b9-a12c-9f135f1f4e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.77.1)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.112.2.tar.gz (579 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m579.2/579.2 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.21.13)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.22.2)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.19.4)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.4.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.8)\n",
      "Collecting schema\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.13 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.24.13)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->sagemaker) (3.0.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Collecting contextlib2>=0.5.5\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.25.0,>=1.24.13->boto3<2.0,>=1.20.21->sagemaker) (1.26.7)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.112.2-py2.py3-none-any.whl size=796128 sha256=c9bee7e1c269037676743add644defa24d71c31a9b4c77571ed70143d4e2a28c\n",
      "  Stored in directory: /root/.cache/pip/wheels/36/9f/18/06cf3b1b76d5f220e62ab030e576092ea53819ea543ff3e790\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: contextlib2, schema, sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.77.1\n",
      "    Uninstalling sagemaker-2.77.1:\n",
      "      Successfully uninstalled sagemaker-2.77.1\n",
      "Successfully installed contextlib2-21.6.0 sagemaker-2.112.2 schema-0.7.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39965073-4a0e-4987-83e4-55ca329ecc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install warmup_scheduler\n",
    "# !pip install monai\n",
    "# !pip install -U \"python-gdcm\" pydicom pylibjpeg\n",
    "# !pip install -U torchvision\n",
    "# !pip install opencv-python\n",
    "# !pip install opencv-python-headless\n",
    "# !pip install wandb\n",
    "# !pip install nibabel\n",
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import cv2\n",
    "import wandb\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "import math\n",
    "import shutil\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import warnings\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from matplotlib.patches import Rectangle\n",
    "from IPython.display import display_html\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# .dcm handling\n",
    "import pydicom\n",
    "import nibabel as nib\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "# Environment check\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "import albumentations\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, cohen_kappa_score, confusion_matrix\n",
    "\n",
    "# MONAI 3D\n",
    "from monai.transforms import Randomizable, apply_transform\n",
    "from monai.transforms import Compose, Resize, ScaleIntensity, ToTensor, RandAffine\n",
    "from monai.networks.nets import densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    '''Reads in all .csv files.'''\n",
    "    \n",
    "    train = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/train.csv\")\n",
    "    train_bbox = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/train_bounding_boxes.csv\")\n",
    "    test = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/test.csv\")\n",
    "    ss = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/sample_submission.csv\")\n",
    "    \n",
    "    return train, train_bbox, test, ss\n",
    "\n",
    "def get_csv_info(csv, name=\"Default\"):\n",
    "    '''Prints main information for the speciffied .csv file.'''\n",
    "    \n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"Shape:\", csv.shape)\n",
    "    print(f\"Missing Values:\", csv.isna().sum().sum(), \"total missing datapoints.\")\n",
    "    print(\"Columns:\", list(csv.columns), \"\\n\")\n",
    "    \n",
    "    display_html(csv.head())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "# some patients have reverse order for the CT scan, so have a function to check\n",
    "def check_reverse_required(path):\n",
    "    paths = list(path.glob('*'))\n",
    "    paths.sort(key=lambda x:int(x.stem))\n",
    "    z_first = pydicom.dcmread(paths[0]).get(\"ImagePositionPatient\")[-1]\n",
    "    z_last = pydicom.dcmread(paths[-1]).get(\"ImagePositionPatient\")[-1]\n",
    "    if z_last < z_first:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "paths = {\n",
    "    'train': Path('../input/rsna-2022-cervical-spine-fracture-detection/train.csv'),\n",
    "    'train_bbox': Path('../input/rsna-2022-cervical-spine-fracture-detection/train_bounding_boxes.csv'),\n",
    "    'train_images': Path('../input/rsna-2022-cervical-spine-fracture-detection/train_images'),\n",
    "    'train_nifti_segments': Path('../input/rsna-2022-cervical-spine-fracture-detection/segmentations'),\n",
    "    'test_df': Path('../input/rsna-2022-cervical-spine-fracture-detection/test.csv'),\n",
    "    'test_images': Path('../input/rsna-2022-cervical-spine-fracture-detection/test_images')\n",
    "}\n",
    "\n",
    "# === 🐝 W&B ===\n",
    "def save_dataset_artifact(run_name, artifact_name, path, data_type=\"dataset\"):\n",
    "    '''Saves dataset to W&B Artifactory.\n",
    "    run_name: name of the experiment\n",
    "    artifact_name: under what name should the dataset be stored\n",
    "    path: path to the dataset'''\n",
    "    \n",
    "    run = wandb.init(project='RSNA_SpineFructure', \n",
    "                     name=run_name, \n",
    "                     config=CONFIG)\n",
    "    artifact = wandb.Artifact(name=artifact_name, \n",
    "                              type=data_type)\n",
    "    artifact.add_file(path)\n",
    "\n",
    "    wandb.log_artifact(artifact)\n",
    "    wandb.finish()\n",
    "    print(\"Artifact has been saved successfully.\")\n",
    "\n",
    "def create_wandb_plot(x_data=None, y_data=None, x_name=None, y_name=None, title=None, log=None, plot=\"line\"):\n",
    "    '''Create and save lineplot/barplot in W&B Environment.\n",
    "    x_data & y_data: Pandas Series containing x & y data\n",
    "    x_name & y_name: strings containing axis names\n",
    "    title: title of the graph\n",
    "    log: string containing name of log'''\n",
    "    \n",
    "    data = [[label, val] for (label, val) in zip(x_data, y_data)]\n",
    "    table = wandb.Table(data=data, columns = [x_name, y_name])\n",
    "    \n",
    "    if plot == \"line\":\n",
    "        wandb.log({log : wandb.plot.line(table, x_name, y_name, title=title)})\n",
    "    elif plot == \"bar\":\n",
    "        wandb.log({log : wandb.plot.bar(table, x_name, y_name, title=title)})\n",
    "    elif plot == \"scatter\":\n",
    "        wandb.log({log : wandb.plot.scatter(table, x_name, y_name, title=title)})\n",
    "        \n",
    "def create_wandb_hist(x_data=None, x_name=None, title=None, log=None):\n",
    "    '''Create and save histogram in W&B Environment.\n",
    "    x_data: Pandas Series containing x values\n",
    "    x_name: strings containing axis name\n",
    "    title: title of the graph\n",
    "    log: string containing name of log'''\n",
    "    \n",
    "    data = [[x] for x in x_data]\n",
    "    table = wandb.Table(data=data, columns=[x_name])\n",
    "    wandb.log({log : wandb.plot.histogram(table, x_name, title=title)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# custom weighted loss function\n",
    "# From: https://www.kaggle.com/code/andradaolteanu/rsna-fracture-detect-pytorch-densenet-train#2.-Data-Split\n",
    "def get_custom_loss(logits, targets):\n",
    "    \n",
    "    # Compute the weights\n",
    "    weights = targets * competition_weights['+'] + (1 - targets) * competition_weights['-']\n",
    "    \n",
    "    # Losses on label and exam level\n",
    "    L = torch.zeros(targets.shape, device=DEVICE)\n",
    "\n",
    "    w = weights\n",
    "    y = targets\n",
    "    p = logits\n",
    "    eps=1e-8\n",
    "\n",
    "    for i in range(L.shape[0]):\n",
    "        for j in range(L.shape[1]):\n",
    "            L[i, j] = -w[i, j] * (\n",
    "                y[i, j] * math.log(p[i, j] + eps) +\n",
    "                (1 - y[i, j]) * math.log(1 - p[i, j] + eps))\n",
    "            \n",
    "    # Average Loss on Exam (or patient)\n",
    "    Exams_Loss = torch.div(torch.sum(L, dim=1), torch.sum(w, dim=1))\n",
    "    \n",
    "    return Exams_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Environment check\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "CONFIG = {'competition': 'RSNA_SpineFracture', '_wandb_kernel': 'aot'}\n",
    "\n",
    "# set seed\n",
    "set_seed(0)\n",
    "\n",
    "# set GPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Kaggle Notebook Setup\n",
    "DF_SIZE = 1\n",
    "N_SPLITS = 5\n",
    "KERNEL_TYPE = 'densenet121_baseline'\n",
    "IMG_RESIZE = 150\n",
    "STACK_RESIZE = 50\n",
    "use_amp = False\n",
    "NUM_WORKERS = 1\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.0005\n",
    "OUT_DIM = 8\n",
    "EPOCHS = 5\n",
    "\n",
    "target_cols = ['C1', 'C2', 'C3', \n",
    "               'C4', 'C5', 'C6', 'C7',\n",
    "               'patient_overall']\n",
    "\n",
    "competition_weights = {\n",
    "    '-' : torch.tensor([1, 1, 1, 1, 1, 1, 1, 7], dtype=torch.float, device=DEVICE),\n",
    "    '+' : torch.tensor([2, 2, 2, 2, 2, 2, 2, 14], dtype=torch.float, device=DEVICE),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>patient_overall</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.6200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.27262</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.21561</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.826.0.1.3680043.12351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2.826.0.1.3680043.1363</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7\n",
       "0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0\n",
       "1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0\n",
       "2  1.2.826.0.1.3680043.21561                1   0   1   0   0   0   0   0\n",
       "3  1.2.826.0.1.3680043.12351                0   0   0   0   0   0   0   0\n",
       "4   1.2.826.0.1.3680043.1363                1   0   0   0   0   1   0   0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "df = pd.read_csv(\"/root/input/rsna-2022-cervical-spine-fracture-detection/train.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size: (2019, 9)\n",
      "K Folds Count:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    404\n",
       "3    404\n",
       "0    404\n",
       "2    404\n",
       "4    403\n",
       "Name: fold, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample down df\n",
    "instances = df.StudyInstanceUID.unique().tolist()\n",
    "instances = random.sample(instances, k=int(len(instances)*DF_SIZE))\n",
    "df = df[df[\"StudyInstanceUID\"].isin(instances)].reset_index(drop=True)\n",
    "print(\"Dataframe size:\", df.shape)\n",
    "\n",
    "# Create folds\n",
    "kfold = GroupKFold(n_splits=N_SPLITS)\n",
    "df['fold'] = -1\n",
    "\n",
    "# Append fold\n",
    "for k, (_, valid_i) in enumerate(kfold.split(df,\n",
    "                                             groups=df.StudyInstanceUID)):\n",
    "    df.loc[valid_i, 'fold'] = k\n",
    "    \n",
    "print(\"K Folds Count:\")\n",
    "df[\"fold\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack all scans together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of scans: 406\n",
      "dimension of each scan: (512, 512, 3)\n",
      "dimension of each scan after stack: (3, 512, 512, 406)\n"
     ]
    }
   ],
   "source": [
    "# take a look at an example\n",
    "study_paths = glob(f\"/root/input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/*\")\n",
    "study_paths.sort(key=natural_keys)\n",
    "        \n",
    "# Load images\n",
    "#OpenCV use BGR format so we need to transfer RGB to BRG\n",
    "study_images = [cv2.imread(path)[:,:,::-1] for path in study_paths]\n",
    "\n",
    "print(\"# of scans:\", len(study_images))\n",
    "print(\"dimension of each scan:\", study_images[0].shape)\n",
    "\n",
    "# stack images\n",
    "stacked_image = np.stack([img.astype(np.float32) for img in study_images], \n",
    "                                 axis=2).transpose(3,0,1,2)\n",
    "\n",
    "print(\"dimension of each scan after stack:\", stacked_image.shape)\n",
    "\n",
    "#show the stacked image \n",
    "# cv2.imshow('image', stacked_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RSNADataset(Dataset, Randomizable):\n",
    "    \n",
    "    def __init__(self, csv, mode, transform=None):\n",
    "        self.csv = csv\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "    \n",
    "    def randomize(self) -> None:\n",
    "        '''-> None is a type annotation for the function that states \n",
    "        that this function returns None.'''\n",
    "        \n",
    "        MAX_SEED = np.iinfo(np.uint32).max + 1\n",
    "        self.seed = self.R.randint(MAX_SEED, dtype=\"uint32\")\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Set Random Seed\n",
    "        self.randomize()\n",
    "        \n",
    "        dt = self.csv.iloc[index, :]\n",
    "        study_paths = glob(f\"/root/input/rsna-2022-cervical-spine-fracture-detection/train_images/{dt.StudyInstanceUID}/*\")\n",
    "        study_paths.sort(key=natural_keys)\n",
    "        \n",
    "        # Load images\n",
    "        study_images = [cv2.imread(path)[:,:,::-1] for path in study_paths]\n",
    "        # Stack all scans into 1\n",
    "        stacked_image = np.stack([img.astype(np.float32) for img in study_images], \n",
    "                                 axis=2).transpose(3,0,1,2)\n",
    "        \n",
    "        if self.transform:\n",
    "            if isinstance(self.transform, Randomizable):\n",
    "                self.transform.set_random_state(seed=self.seed)\n",
    "                \n",
    "            stacked_image = apply_transform(self.transform, stacked_image)\n",
    "        \n",
    "        if self.mode==\"test\":\n",
    "            return {\"image\": stacked_image}\n",
    "        else:\n",
    "            targets = torch.tensor(dt[target_cols]).float()\n",
    "            return {\"image\": stacked_image,\n",
    "                    \"targets\": targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send the data to GPU\n",
    "def data_to_device(data):\n",
    "    \n",
    "    image, targets = data.values()\n",
    "    return image.to(DEVICE), targets.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transform\n",
    "train_transforms = Compose([ScaleIntensity(), \n",
    "                            Resize((IMG_RESIZE, IMG_RESIZE, STACK_RESIZE)), \n",
    "                            # TODO - add more here\n",
    "                            ToTensor()])\n",
    "valid_transforms = Compose([ScaleIntensity(), \n",
    "                          Resize((IMG_RESIZE, IMG_RESIZE, STACK_RESIZE)), \n",
    "                          ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \n",
      "Image: (3, 3, 150, 150, 50) \n",
      "Targets: tensor([[1., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1.]], device='cuda:0') \n",
      "==================================================\n",
      "Batch: 1 \n",
      "Image: (3, 3, 150, 150, 50) \n",
      "Targets: tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1.]], device='cuda:0') \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "\n",
    "# Sample data\n",
    "sample_df = df.head(6)\n",
    "\n",
    "# Instantiate Dataset object\n",
    "dataset = RSNADataset(csv=sample_df, mode=\"train\", transform=train_transforms)\n",
    "# The Dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "# Output of the Dataloader\n",
    "for k, data in enumerate(dataloader):\n",
    "    image, targets = data_to_device(data)\n",
    "    print( f\"Batch: {k}\", \"\\n\" +\n",
    "          \"Image:\", image.shape, \"\\n\" +\n",
    "          \"Targets:\", targets, \"\\n\" +\n",
    "          \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dataset, dataloader, image, targets\n",
    "\n",
    "# gabage collector which is used to free up memory, return the counts of objects \n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss & Gradual warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference link: [HERE](https://stackoverflow.com/questions/42479902/what-does-view-do-in-pytorch)\n",
    "\n",
    "torch.view(-1):\n",
    "* view() reshapes the tensor without copying memory, similar to numpy's reshape().\n",
    "* -1 flatten the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CRITERION = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "def get_criterion(logits, target): \n",
    "    loss = CRITERION(logits.view(-1), target.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    '''\n",
    "    src: https://www.kaggle.com/code/boliu0/monai-3d-cnn-training/notebook\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, \n",
    "                                                       total_epoch, after_scheduler)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier \n",
    "                                                     for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        \n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) \n",
    "                    for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) \n",
    "                    for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log the info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_in_file(text, f):\n",
    "    \n",
    "    with open(f'log_{KERNEL_TYPE}.txt', 'a+') as f:\n",
    "        print(text, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, epoch, f):\n",
    "    \n",
    "    # Add info to file\n",
    "    print(\"Training...\")\n",
    "    add_in_file('Training...', f)\n",
    "    \n",
    "    # Track training time for 1 epoch\n",
    "    start_time = time()\n",
    "    \n",
    "    # === TRAIN ===\n",
    "    model.train()\n",
    "    train_losses, train_comp_losses = [], []\n",
    "    \n",
    "    # Loop through the data\n",
    "    bar = tqdm(dataloader)\n",
    "    for data in bar:\n",
    "        image, targets = data_to_device(data)\n",
    "        \n",
    "        # Train & Optimize\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(image)\n",
    "        loss = get_criterion(logits, targets)\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # === COMP LOSS ===\n",
    "        comp_loss = get_custom_loss(logits, targets)\n",
    "\n",
    "        # Save losses\n",
    "        train_losses.append(loss.detach().cpu().numpy())\n",
    "        train_comp_losses.append(comp_loss.detach().cpu().numpy().mean())\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "    # Compute Overall Loss\n",
    "    mean_train_loss = np.mean(train_losses)\n",
    "    mean_comp_loss = np.mean(train_comp_losses)\n",
    "    \n",
    "    # Save info\n",
    "    total_time = round((time() - start_time)/60, 3)\n",
    "    add_in_file('Train Mean Loss: {}'.format(mean_train_loss), f)\n",
    "    add_in_file('Train Mean Comp Loss: {}'.format(mean_comp_loss), f)\n",
    "    add_in_file('~~~ Train Time: {} mins ~~~'.format(total_time), f)\n",
    "    \n",
    "    # 🐝 Log to W&B\n",
    "    wandb.log({\"train_loss\": mean_train_loss,\n",
    "               \"train_comp_loss\": mean_comp_loss,}, step=epoch)\n",
    "                \n",
    "    # Print info\n",
    "    print(\"Train Mean Loss:\", mean_train_loss)\n",
    "    print(\"Train Mean Comp Loss:\", mean_comp_loss)\n",
    "    print(f\"~~~ Train Time: {total_time} mins ~~~\")\n",
    "    \n",
    "    return mean_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_epoch(model, dataloader, epoch, f):\n",
    "    \n",
    "    # Add info to file\n",
    "    print(\"Validation...\")\n",
    "    add_in_file('Validation...', f)\n",
    "    \n",
    "    # Track validation time for 1 epoch\n",
    "    start_time = time()\n",
    "    \n",
    "    # === EVAL ===\n",
    "    model.eval()\n",
    "    valid_preds, valid_targets, valid_comp_loss = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            \n",
    "            image, targets = data_to_device(data)\n",
    "            logits = model(image)\n",
    "            \n",
    "            # === COMP LOSS ===\n",
    "            comp_loss = get_custom_loss(logits, targets)\n",
    "            # Save actuals, preds and losses\n",
    "            valid_targets.append(targets.detach().cpu())\n",
    "            valid_preds.append(logits.detach().cpu())\n",
    "            valid_comp_loss.append(comp_loss.detach().cpu().numpy().mean())\n",
    "            \n",
    "            gc.collect()\n",
    "            \n",
    "    # Overall Valid Loss\n",
    "    valid_losses = get_criterion(torch.cat(valid_preds), torch.cat(valid_targets)).numpy()\n",
    "    mean_valid_loss = np.mean(valid_losses)\n",
    "    \n",
    "    # Overall Competition Loss\n",
    "    mean_comp_valid_loss = np.mean(valid_comp_loss)\n",
    "    \n",
    "    # Compute Area Under Curve\n",
    "    PREDS = np.concatenate(torch.cat(valid_preds).numpy())\n",
    "    TARGETS = np.concatenate(torch.cat(valid_targets).numpy())\n",
    "    auc = roc_auc_score(TARGETS, PREDS)\n",
    "    \n",
    "    # Save info\n",
    "    total_time = round((time() - start_time)/60, 3)\n",
    "    add_in_file('Valid Mean Loss: {}'.format(mean_valid_loss), f)\n",
    "    add_in_file('Valid Mean Comp Loss: {}'.format(mean_comp_valid_loss), f)\n",
    "    add_in_file('Valid AUC: {}'.format(auc), f)\n",
    "    add_in_file('~~~ Valid Time: {} mins ~~~'.format(total_time), f)\n",
    "    \n",
    "    # 🐝 Log to W&B\n",
    "    wandb.log({\"valid_loss\": mean_valid_loss,\n",
    "               \"valid_comp_loss\": mean_comp_valid_loss,\n",
    "               \"valid_auc\": auc}, step=epoch)\n",
    "        \n",
    "    # Print info\n",
    "    print(\"Valid Mean Loss:\", mean_valid_loss)\n",
    "    print(\"Valid Mean Comp Loss:\", mean_comp_valid_loss)\n",
    "    print(\"Valid AUC:\", auc)\n",
    "    print(f\"~~~ Validation Time: {total_time} mins ~~~\")\n",
    "    \n",
    "    return mean_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_train(fold):\n",
    "    \n",
    "    # 🐝 W&B Tracking\n",
    "    RUN_CONFIG = CONFIG.copy()\n",
    "    params = dict(model=\"densenet121\", \n",
    "                  epochs=EPOCHS, \n",
    "                  split=N_SPLITS, \n",
    "                  batch=BATCH_SIZE, lr=LR,\n",
    "                  img_size=IMG_RESIZE, stack_size=STACK_RESIZE,\n",
    "                  data_size=DF_SIZE)\n",
    "    RUN_CONFIG.update(params)\n",
    "    run = wandb.init(project='RSNA_SpineFracture', config=CONFIG)\n",
    "    \n",
    "    # Get the train and valid data\n",
    "    train = df[df[\"fold\"] != fold].reset_index(drop=True)\n",
    "    valid = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "    \n",
    "    # Create the Dataset & Dataloader\n",
    "    train_dataset = RSNADataset(csv=train, mode=\"train\", \n",
    "                                transform=train_transforms)\n",
    "    valid_dataset = RSNADataset(csv=valid, mode=\"train\", \n",
    "                                transform=valid_transforms)\n",
    "    trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                             sampler=RandomSampler(train_dataset))\n",
    "    validloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Model\n",
    "    model = densenet.densenet121(spatial_dims=3, in_channels=3,\n",
    "                                 out_channels=OUT_DIM)\n",
    "    model.class_layers.out = nn.Sequential(nn.Linear(in_features=1024, out_features=OUT_DIM), \n",
    "                                           nn.Softmax(dim=1))\n",
    "    model.to(DEVICE)\n",
    "    wandb.watch(model, log_freq=100) # 🐝\n",
    "    \n",
    "    # Optimizer & Scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler_cosine = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 2)\n",
    "    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, \n",
    "                                                total_epoch=1, \n",
    "                                                after_scheduler=scheduler_cosine)\n",
    "    \n",
    "    # Initiate initial loss\n",
    "    valid_loss_BEST = 1000\n",
    "    # Create model name\n",
    "    model_file = f'{KERNEL_TYPE}_best_fold{fold}.pth'\n",
    "    # Create file to save outputs\n",
    "    f = open(f'log_{KERNEL_TYPE}.txt', 'a')\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        add_in_file('======== Epoch: {}/{} ========'.format(epoch+1, EPOCHS), f)\n",
    "        print(\"=\"*8, f\"Epoch {epoch}\", \"=\"*8)\n",
    "        \n",
    "        scheduler_warmup.step(epoch-1)\n",
    "        \n",
    "        # Train & Validate\n",
    "        mean_train_loss = train_epoch(model, trainloader, optimizer, epoch, f)\n",
    "        mean_valid_loss = valid_epoch(model, validloader, epoch, f)\n",
    "        \n",
    "        # Save model\n",
    "        if mean_valid_loss < valid_loss_BEST:\n",
    "            print('Saving model ...')\n",
    "            add_in_file('Saving model => {}'.format(model_file), f)\n",
    "            torch.save(model.state_dict(), model_file)\n",
    "            valid_loss_BEST = mean_valid_loss\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # 🐝 Experiment End\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-09 23:58:22,853 - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: fengyaoluo. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/workspace/UCB-w210-2022-fall-neckfrac/wandb/run-20221009_235824-2x8cius9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fengyaoluo/RSNA_SpineFracture/runs/2x8cius9\" target=\"_blank\">toasty-capybara-13</a></strong> to <a href=\"https://wandb.ai/fengyaoluo/RSNA_SpineFracture\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 0 ========\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "run_train(fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 🐝 Save Artifacts\n",
    "save_dataset_artifact(run_name=\"save_logs\", artifact_name=\"logs\",\n",
    "                      path=\"/root/workspace/UCB-w210-2022-fall-neckfrac/log_densenet121_baseline.txt\", data_type=\"dataset\")\n",
    "save_dataset_artifact(run_name=\"save_model\", artifact_name=\"model\",\n",
    "                      path=\"/root/workspace/UCB-w210-2022-fall-neckfrac/densenet121_baseline_best_fold0.pth\", data_type=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [RSNA Fracture Detect: PyTorch DenseNet train](https://www.kaggle.com/code/andradaolteanu/rsna-fracture-detect-pytorch-densenet-train)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
